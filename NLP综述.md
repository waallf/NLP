## 分布式向量或词嵌入向量基本上遵循分布式假设，即具有相似语义的词倾向于具有相似的上下文词  
## 分布式表示（词嵌入）主要通过上下文或者词的「语境」来学习本身该如何表达.  
## 128、256、300、500 等  
## 词嵌入的一个局限是它们无法表示短语（Mikolov et al., 2013），
即两个词或多个词的组合并不表示对应的短语意义，例如「人民」+「大学」并不能组合成「人民大学」  
解决方法：1. 基于词共现识别这些短语，并为它们单独地学一些词嵌入向量  
         2. 学习 n-gram 词嵌入
         
## 情感词嵌入（SSWE）：在学习嵌入时，将损失函数中的监督情感纳入其中。
## CNN 在上下文窗口中挖掘语义信息非常有效，然而它们是一种需要大量数据训练大量参数的模型。因此在数据量不够的情况下，CNN 的效果会显著降低。
CNN 另一个长期存在的问题是它们无法对长距离上下文信息进行建模并保留序列信息，其它如递归神经网络等在这方面有更好的表现。  
## RNN 通过建模序列中的单元来处理序列，它能够捕获到语言中的内在序列本质。  
## 这种处理任意长度输入的能力是使用 RNN 的主要研究的卖点之一。  

## 近期，多项研究就 CNN 优于 RNN 提出了证据。甚至在 RNN 适合的语言建模等任务中，CNN 的性能与 RNN 相当。CNN 与 RNN 在建模句子时的目标函数不同。
RNN 尝试建模任意长度的句子和无限的上下文，而 CNN 尝试提取最重要的 n-gram。尽管研究证明 CNN 是捕捉 n-gram 特征的有效方式，这在特定长度的句子分
类任务中差不多足够了，但 CNN 对词序的敏感度有限，容易限于局部信息，忽略长期依赖。

# 相关的数据集：  
WSJ-PTB  语料库包含 117 万个 token  用于词性标注  

CoNLL 2003 四种命名实体：人、地点、组织和其它实体  用于命名实体识别（NER）  

语义角色标注（SRL） 标注 施事者、受事者、工具等，以及地点、时间、方式、原因等修饰语  

SST 数据集  进行情感分析  
